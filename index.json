[{"authors":null,"categories":null,"content":"My Chinese name is 于(Yu)鹏(Peng)飞(fei). I am a PhD student in computer science at UIUC blender lab advised by Heng Ji. My research interests include information extraction (IE) and its applications in NLP. I also have some experience on a variety of topics, such as meta learning, transfer learning, continual learning, weakly-supervised learning and multimodal learning.\n","date":1656633600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656633600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My Chinese name is 于(Yu)鹏(Peng)飞(fei). I am a PhD student in computer","tags":null,"title":"Pengfei Yu","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://perfec-yu.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Pengfei Yu","Zixuan Zhang","Clare Voss","Jonathan May","Heng Ji"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"3d89774d919c16ace6b79dc80a4f160f","permalink":"https://perfec-yu.github.io/publication/yu-etal-2022-event/","publishdate":"2022-06-25T20:43:20.041226Z","relpermalink":"/publication/yu-etal-2022-event/","section":"publication","summary":"","tags":null,"title":"Event Extractor with Only a Few Examples","type":"publication"},{"authors":["Xinya Du","Zixuan Zhang","Sha Li","Pengfei Yu","Hongwei Wang","Tuan Manh","Xudong Lin","Ziqi Wang","Iris Liu","Ben Zhou","Haoyang Wen","Manling Li","Darryl Hannan","Qi Zeng","Qing Lyu","Charles Yu","Carl Edwards","Xiaomeng Jin","Yizhu Jiao","Ghazaleh Kazeminejad","Rotem Dror","Zhenhailong Wang","Chris Callison-Burch","Mohit Bansal","Carl Vondrick","Jiawei Han","Dan Roth","Shih-Fu Chang","Martha Palmer","Heng Ji"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b5cd5af43a60aaa8de259aa3e4eaff35","permalink":"https://perfec-yu.github.io/publication/du-etal-2022-resin/","publishdate":"2022-06-25T20:43:20.042461Z","relpermalink":"/publication/du-etal-2022-resin/","section":"publication","summary":"","tags":null,"title":"RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios","type":"publication"},{"authors":["Manling Li","Revanth Gangi Reddy","Ziqi Wang","Yi-shyuan Chiang","Tuan Lai","Pengfei Yu","Zixuan Zhang","Heng Ji"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"fe60d84adb5c84109cda7c7a2d97a8a7","permalink":"https://perfec-yu.github.io/publication/li-etal-2022-covid/","publishdate":"2022-06-25T20:43:20.043176Z","relpermalink":"/publication/li-etal-2022-covid/","section":"publication","summary":"To tackle the challenge of accurate and timely communication regarding the COVID-19 pandemic, we present a COVID-19 Claim Radar to automatically extract supporting and refuting claims on a daily basis. We provide a comprehensive structured view of claims, including rich claim attributes (such as claimers and claimer affiliations) and associated knowledge elements as claim semantics (such as events, relations and entities), enabling users to explore equivalent, refuting, or supporting claims with structural evidence, such as shared claimers, similar centroid events and arguments. In order to consolidate claim structures at the corpus-level, we leverage Wikidata as the hub to merge coreferential knowledge elements. The system automatically provides users a comprehensive exposure to COVID-19 related claims, their importance, and their interconnections. The system is publicly available at GitHub and DockerHub, with complete documentation.","tags":null,"title":"COVID-19 Claim Radar: A Structured Claim Extraction and Tracking System","type":"publication"},{"authors":["Pengfei Yu","Heng Ji","Prem Natarajan"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f2c54e1633764b9a72306aa1cf5d88f8","permalink":"https://perfec-yu.github.io/publication/yu-etal-2021-lifelong/","publishdate":"2022-06-25T20:43:20.043856Z","relpermalink":"/publication/yu-etal-2021-lifelong/","section":"publication","summary":"Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or semantic relatedness exist among hierarchical knowledge element types. In our proposed framework, knowledge is being transferred between learned old event types and new event types. Specifically, we update old knowledge with new event types′ mentions using a self-training loss. In addition, we aggregate old event types′ representations based on their similarities with new event types to initialize the new event types′ representations. Experimental results show that our framework outperforms competitive baselines with a 5.1% absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30% absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel knowledge acquisition.","tags":null,"title":"Lifelong Event Detection with Knowledge Transfer","type":"publication"},{"authors":["Xiaodan Hu","Pengfei Yu","Kevin Knight","Heng Ji","Bo Li","Honghui Shi"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"c2f0fce9c3de5813638ba4d7e1d7a572","permalink":"https://perfec-yu.github.io/publication/xiaodan-2021-muse/","publishdate":"2022-06-25T20:43:20.044556Z","relpermalink":"/publication/xiaodan-2021-muse/","section":"publication","summary":"","tags":null,"title":"MUSE: Textual Attributes Guided Portrait Painting Generation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://perfec-yu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Xu Han","Hao Zhu","Pengfei Yu","Ziyun Wang","Yuan Yao","Zhiyuan Liu","Maosong Sun"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"83dd5e9587c1af50cf495437dbbae9ea","permalink":"https://perfec-yu.github.io/publication/han-etal-2018-fewrel/","publishdate":"2022-06-25T20:43:20.045135Z","relpermalink":"/publication/han-etal-2018-fewrel/","section":"publication","summary":"We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.","tags":null,"title":"FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation","type":"publication"},{"authors":["Xu Han","Pengfei Yu","Zhiyuan Liu","Maosong Sun","Peng Li"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"5dec288ac7b6a412a6d795603aaadf35","permalink":"https://perfec-yu.github.io/publication/han-etal-2018-hierarchical/","publishdate":"2022-06-25T20:43:20.045908Z","relpermalink":"/publication/han-etal-2018-hierarchical/","section":"publication","summary":"Distantly supervised relation extraction employs existing knowledge graphs to automatically collect training data. While distant supervision is effective to scale relation extraction up to large-scale corpora, it inevitably suffers from the wrong labeling problem. Many efforts have been devoted to identifying valid instances from noisy data. However, most existing methods handle each relation in isolation, regardless of rich semantic correlations located in relation hierarchies. In this paper, we aim to incorporate the hierarchical information of relations for distantly supervised relation extraction and propose a novel hierarchical attention scheme. The multiple layers of our hierarchical attention scheme provide coarse-to-fine granularity to better identify valid instances, which is especially effective for extracting those long-tail relations. The experimental results on a large-scale benchmark dataset demonstrate that our models are capable of modeling the hierarchical information of relations and significantly outperform other baselines. The source code of this paper can be obtained from r̆lhttps://github.com/thunlp/HNRE.","tags":null,"title":"Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention","type":"publication"},{"authors":["Yinpei Dai","Zhijian Ou","Dawei Ren","Pengfei Yu"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c5d5ed7eac96896b9f682360be5fdc59","permalink":"https://perfec-yu.github.io/publication/dai-2018-tracking/","publishdate":"2022-06-25T20:43:20.046658Z","relpermalink":"/publication/dai-2018-tracking/","section":"publication","summary":"","tags":null,"title":"Tracking of Enriched Dialog States for Flexible Conversational Information Access","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://perfec-yu.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":["Pengfei Yu"],"categories":["Ideas"],"content":"Why Writing This and What This Is About TLDR: Why? Call for broader discussion and potential collaboration to improve and implement this idea. What? About building a new framework that is not based on a constant language model, but a stochastic language model. This new framework explains a lot challenges and recent work on LLMs and provides insights on improving langauge learning. Then click there to the main content\nI recently came up with notion of modeling language not as a constant language model, but a random variable or a stoachastic language model. This idea originates from many things I have worked on, am working on and have read about. Also, it is not a completely new idea, instead it is related, similar to, and combines several established methods and mathematical models, though with minor differences to each of them. In fact I wouldn’t even expect any framework that completely negates everything intelligent researchers have built for AI and machine learning. Anyway, I have been trying to formulate this stochastic language model and see what kind of insights we can get from this perspective. Interestingly, as I am developing this framework, I notice this is closely related a lot challenges or methodlogies in current LLM research, including but not limited to hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation, mixture-of-experts and multimodality (the connection between randomizing a language model and multimodality sounds a little weird, but I will explain later).\nCurrently, I have sort out the most basic concepts of what it means to randomize a language model, what are its implications\u0026amp;benefits and roughly how we should build this model or improve existing LLMs by approximating this model. However, I found that it is not trivial to explain this idea to others in a short, e.g., 30min to 1h, period of time and using more brief media of communications such as slides. I was only able to convey and improve most of my thoughts through multiple conversations and active discussions with some of my collegue PhD students. However, due to the level of generality as it seems to me for now, I think this notion of stochastic language modeling is kind of useful for reaserch in LLMs. Moreover, limited by my personal mathematical capability and available computational and data resources, it seems very challenging to fully implement everything I am thinking of under this framework. Moreover, I think many on-going research in the community could be closely related to this framework. So I think maybe it is simply better to write about this idea for broader discussions, suggestions and potential collaboration to further improve this idea and bring this idea to reality. Another good thing about a post is that it doesn’t have to like research papers or a set of slides that need to follow certain structures. I feel more comfortable talking about my thoughts in this style.\nIn this post, I will try my best, although it might be kind of hard, to present this idea in an organized way. It might happen that you get a little bit lost at the start, but if you keep reading, it is very possible at some point that at some point you realize “oh this is related to/ kind of explains what I am working on / have worked on”. I would be very happy if these kind of situations happen and if you are interested in further discussion on this idea, please reach out at pengfei4 at illinois dot edu.\nOverview I will try to explain this idea in the following aspects.\nBasic mathematical definition of the stochastic language modeling. I shall use the name Stochastic Procedural Language Modeling or SPLM since it is better to explain as a stochastic process rather than a single random variable. I know it could be elusive and boring to start with math rather than vivid examples, but I found that even a basic notion of what we are aiming for will benefit the understanding the “vivid examples”. The math part will not go beyond undergraduate-level probability theory, since I will not go into deeper analysis of the stochastic process. The purpose is just to leave a basic impression of the objective. How this framework gives better explaination of various challenges mentioned above in LLMs, and how it is potentially easier to solve such challenges under this framework. Explaning that a series of augmentation methods proposed for LLM are trying to approximate this framework. Moreover, how this framework explains an incomplete collection recent work. Some initial thoughts on how to implement this idea, and how flexible this implemented framework will be in representing not only language but many intelligent behaviors. (to-be finished) Definition of Stochstic Procedural Language Model Conventions for Notations Lower case letters for variables Upper letters for mappings (functions, distributions, etc.) Caligraphic letters for sets or spaces, except for conventional …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4b9d0265a52749f3a7867809454c46d5","permalink":"https://perfec-yu.github.io/post/splm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/splm/","section":"post","summary":"Considering language models as random variables brings insights into the challenges in LLMs and the mechanisms of various augmentation methods to improve LLMs, such has hallucination, biases, continual fine-tuning/knowledge editing/instruction fine-tuning/RLHF, tool learning, role playing, retrieval augmentation and many other things...","tags":["Academic"],"title":"Thoughts on Randomizing Lanugage Modeling for Better Language Learning","type":"post"}]