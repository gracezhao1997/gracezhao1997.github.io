<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Min Zhao (ËµµÊïè)</title>

  <meta name="author" content="Min Zhao">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">



<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Min Zhao (ËµµÊïè)</name>
                  </p>

                  <p align="justify">
                    I am a postdoctoral researcher at Tsinghua University, working under the supervision of Prof. <a href="http://ml.cs.tsinghua.edu.cn/~jun/index.shtml" target="_blank" rel="noopener"> Jun Zhu</a>. I have collaborated closely with Prof. <a href="https://zhenxuan00.github.io/" target="_blank" rel="noopener"> Chongxuan Li</a> and Dr. <a href="https://baofff.github.io/" target="_blank" rel="noopener"> Fan Bao</a>. I received my Ph.D. degree from Chinese Academy of Sciences Institute of Automation, advised by Prof. <a href="https://brain.bnu.edu.cn/English/Faculty/CurrentFaculty/Szz/d868795bf21341afbcfdddbc4d2b0063.htm" target="_blank" rel="noopener"> Jing Sui</a> and Prof. <a href="https://people.ucas.ac.cn/~yushan?language=en" target="_blank" rel="noopener"> Shan Yu</a>. Before that, I received my B.S. degree from Beijing Institute of Technology in 2019.                 </p>                  
<p align="justify">         
My research interests include AIGC, with a particular focus on image, video, and 3D generation using diffusion models. Feel free to connect with me for research collaborations or discussions related to the above topics.                  </p>

                  <p style="text-align:center">
                    <a href="mailto:gracezhao1997@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ExIZrLAAAAAJ&hl=zh-CN&oi=sra">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/gracezhao1997"> GitHub </a> &nbsp/&nbsp
		<a href="https://www.xiaohongshu.com/user/profile/5a2f9900db2e6048481c2359?tab=note&subTab=note"> Â∞èÁ∫¢‰π¶ </a> &nbsp/&nbsp
	        <a href="https://github.com/gracezhao1997/gracezhao1997.github.io/blob/main/slides/Min_s_Notes.pdf"> Á¨îËÆ∞ </a> 
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <table align="center" border="0" cellpadding="10" cellspacing="0" width="100%">
            <tbody>
              <tr>
                <td valign="middle" width="100%">
                  <heading>News </heading>
                </td>
              </tr>
            </tbody>
          </table>


          <ul>
			  <li><span style="color: gray" size="6px">[2025.09]</span>
               üéâ Our paper <a href="https://arxiv.org/pdf/2503.13265"><strong>FlexWorld</strong></a>, which expands 3D Scenes for flexiable-View synthesis, is accepted by <strong>NeurIPS 2025</strong>. </li>
            <li><span style="color: gray" size="6px">[2025.05]</span>
               üéâ Our paper <a href="https://arxiv.org/abs/2502.15894"><strong>RIFLEx</strong></a>, which extends video with just one line of code, has been accepted by <strong>ICML 2025</strong>. It now supports various models including <a href="https://github.com/Tencent/HunyuanVideo"><strong>HunyuanVideo</strong></a>, <a href="https://github.com/THUDM/CogVideo"><strong>CogVideoX</strong></a>, and <a href="https://github.com/Wan-Video/Wan2.1"><strong>Wan2.1</strong></a>, extending video lengths from 5-6 s to 10-12 s.  </li>

	    <li><span style="color: gray" size="6px">[2024.09]</span>
               üéâ One paper: <a href="https://arxiv.org/pdf/2406.15735"><strong>Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model</strong></a> is accepted by <strong>NeurIPS 2024</strong>.            </li>

            <li><span style="color: gray" size="6px">[2024.07]</span>
               üéâ One paper: <a href="https://arxiv.org/abs/2405.14582"><strong>PoseCrafter</strong></a> is accepted by <strong>ECCV 2024</strong>.
            </li>

            <li><span style="color: gray" size="6px">[2024.05]</span>
               üéâ Our text-to-video model <a href="https://www.shengshu-ai.com/vidu"><strong>Vidu</strong></a>, which produces <strong>16s clips with 1080p</strong> and is similar to <a href="https://openai.com/index/sora/"><strong>Sora</strong></a>, has been published.
            </li>

            <li><span style="color: gray" size="6px">[2023.01]</span>
               üéâ One paper: <a href="https://arxiv.org/abs/2209.15408"><strong>EEGSDE</strong></a> is accepted by <strong>ICLR 2023</strong>.
            </li>

            <li><span style="color: gray" size="6px">[2022.09]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2207.06635"><strong>EGSDE</strong></a> is accepted by <strong>NeurIPs 2022</strong>.
            </li>
         
  
          </ul>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Selected Publications</heading>
                </td>
         
              </tr>
            </tbody>
      
          </table>
         

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

			<tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/FlexWorld.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis
                    </papertitle>
                  </a>
                  <br /><br />
                  Luxi Chen, Zihan Zhou, <strong>Min Zhao</strong>, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, Chongxuan Li, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, Jun Zhu
                  <br /><br />
                  <strong>NeurIPS 2025</strong>
                  <br>
                  <a href="https://arxiv.org/pdf/2503.13265">Paper</a>&nbsp/&nbsp
                  <a href="https://ml-gsai.github.io/FlexWorld/">Website</a>&nbsp/&nbsp
                  <a href="https://github.com/ML-GSAI/FlexWorld">Code</a><img src="https://img.shields.io/github/stars/thu-ml/riflex?style=social">
                  </p>
                </td>
            </tr>

            <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/riflex.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Min Zhao</strong>,
                  Guande He,
                  Yixiao Chen,
                  Hongzhou Zhu,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>ICML 2025</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2502.15894">Paper</a>&nbsp/&nbsp
                  <a href="https://riflex-video.github.io/">Website</a>&nbsp/&nbsp
                  <a href="https://github.com/thu-ml/RIFLEx">Code</a><img src="https://img.shields.io/github/stars/thu-ml/riflex?style=social">
                  </p>
                </td>
            </tr>

            <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/CIL.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Identifying and Solving Conditional Image Leakage in Image-to-Video Generation
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Min Zhao*</strong>,
                  Hongzhou Zhu*,
                  Chendong Xiang,
                  Kaiwen Zheng,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>NeurIPS 2024</strong>
                  <br>
                  <a href="https://arxiv.org/pdf/2406.15735v1">Paper</a>&nbsp/&nbsp
                  <a href="https://cond-image-leak.github.io/">Website</a>&nbsp/&nbsp
                  <a href="https://github.com/thu-ml/cond-image-leakage/">Code</a><img src="https://img.shields.io/github/stars/thu-ml/cond-image-leakage?style=social">
                  </p>
                </td>
            </tr>

              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/posecrafter.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>PoseCrafter: One-Shot Personalized Video Synthesis Following Flexible Poses
                    </papertitle>
                  </a>
                  <br /><br />
                  Yong Zhong*,
                  <strong>Min Zhao*</strong>,
                  Zebin You,
                  Xiaofeng Yu,
                  Changwang Zhang,
                  Chongxuan Li
                  <br /><br />
                  <strong>ECCV 2024</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2405.14582">Paper</a>&nbsp/&nbsp
                  <a href="https://ml-gsai.github.io/PoseCrafter-demo/">Website</a>
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/vidu.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models
                    </papertitle>
                  </a>
                  <br /><br />
                  Fan Bao,
                  Chendong Xiang*,
                  Gang Yue*,
                  Guande He*,
                  Hongzhou Zhu*,
                  Kaiwen Zheng*,
                  <strong>Min Zhao*</strong>,
                  Shilong Liu*,
                  Yaole Wang*,
                  Jun Zhu
                  <br /><br />
                  <strong>Arixv</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2405.04233">Paper</a>&nbsp/&nbsp
                  <a href="https://www.shengshu-ai.com/vidu">Website</a>
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/controlvideo.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Controlvideo: Adding conditional control for one shot text-to-video editing
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Min Zhao</strong>,
                  Rongzhen Wang,
                  Fan Bao,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>Science China Information Sciences 2024</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2305.17098">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/thu-ml/controlvideo">Code</a><img src="https://img.shields.io/github/stars/thu-ml/controlvideo?style=social">&nbsp/&nbsp
                  <a href="https://ml.cs.tsinghua.edu.cn/controlvideo/">Website</a>&nbsp/&nbsp
                  <a href="https://zhuanlan.zhihu.com/p/669056473">Zhihu</a>
                </td>
              </tr>


            
              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/EEGSDE.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Equivariant Energy-Guided SDE for Inverse Molecular Design
                    </papertitle>
                  </a>
                  <br /><br />
                  Fan Bao*,
                  <strong>Min Zhao*</strong>,
                  Zhongkai Hao,
                  Peiyao Li,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>ICLR 2023</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2209.15408">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/gracezhao1997/EEGSDE">Code</a><img src="https://img.shields.io/github/stars/gracezhao1997/EEGSDE?style=social">&nbsp/&nbsp 
                  <a href="posters/EEGSDE-poster.pdf">Poster</a>&nbsp/&nbsp
                  <a href="slides/EEGSDE_slides.pdf">Slides</a>
                </td>
              </tr>

              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/egsde.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle> EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Min Zhao</strong>,
                  Fan Bao,
                  Chongxuan Li,
                  Jun Zhu
                  <br /><br />
                  <strong>NeurIPS 2022</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2207.06635">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/ML-GSAI/EGSDE">Code</a><img src="https://img.shields.io/github/stars/ML-GSAI/EGSDE?style=social">&nbsp/&nbsp
                  <a href="https://www.bilibili.com/video/BV1xg411h7qj/?spm_id_from=333.788.recommend_more_video.17&vd_source=c44eb1672dfc1883b93289ab00b41382">Talk</a>&nbsp/&nbsp
                  <a href="posters/EGSDE-poster.pdf">Poster</a>&nbsp/&nbsp
                  <a href="slides/egsde_slides.pdf">Slides</a>
                </td>
              </tr>

            <tr>
                  <td style="padding:25px;width:35%;vertical-align:middle">
                    <div class="one">
                      <img src='images/mia.png' width="250">
                    </div>
                  </td>
                  <td style="padding:25px;width:70%;vertical-align:middle">
                    <a href="">
                      <papertitle> An attention-based hybrid deep learning framework integrating brain
connectivity and activity of resting-state functional MRI data
                      </papertitle>
                    </a>
                    <br /><br />
                    <strong>Min Zhao</strong>,
                    Weizheng Yan,
                    Na Luo,
                    Dongmei Zhi,
                    Zening Fu,
                    Yuhui Du,
                    Shan Yu,
                    Tianzi Jiang,
                    Vince D. Calhoun,
                    Jing Sui
                    <br /><br />
                    <strong>Medical Image Analysis</strong>
                    <br>
                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841522000652">Paper</a>&nbsp/&nbsp
                  </td>
                </tr>

            <tr>
                  <td style="padding:25px;width:35%;vertical-align:middle">
                    <div class="one">
                      <img src='images/sr.png' width="250">
                    </div>
                  </td>
                  <td style="padding:25px;width:70%;vertical-align:middle">
                    <a href="">
                      <papertitle> Mapping relationships among schizophrenia, bipolar and schizoaffective disorders: A deep classification and clustering framework using fMRI time series
                      </papertitle>
                    </a>
                    <br /><br />
                    Weizheng Yan*,
                    <strong>Min Zhao*</strong>,
                    Godfrey D. Pearlson,
                    Jing Sui,
                    Vince D. Calhoun
                    <br /><br />
                    <strong>Schizophrenia Research</strong>
                    <br>
                    <a href="https://pubmed.ncbi.nlm.nih.gov/33676821/">Paper</a>&nbsp/&nbsp
                  </td>
                </tr>

              <tr>
                  <td style="padding:25px;width:35%;vertical-align:middle">
                    <div class="one">
                      <img src='images/isbi.png' width="250">
                    </div>
                  </td>
                  <td style="padding:25px;width:70%;vertical-align:middle">
                    <a href="">
                      <papertitle> An attention-based hybrid deep learning framework integrating temporal coherence and dynamics for discriminating schizophrenia
                      </papertitle>
                    </a>
                    <br /><br />
                    <strong>Min Zhao</strong>,
                    Weizheng Yan,
                    Rongtao Xu,
                    Dongmei Zhi,
                    Rongtao Jiang,
                    Tianzi Jiang,
                    Vince D. Calhoun,
                    Jing Sui
                    <br /><br />
                    <strong>IEEE ISBI 2021, <span style="color:red;">Best student paper nomination</span></strong>
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/9433919">Paper</a>&nbsp/&nbsp
                  </td>
                </tr>

          </table>


          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Services</heading>
                <p align="justify"><strong>Reviewers for conferences:</strong> NeurIPs, ICLR , AAAI, ACM MM</p>
		<p align="justify"><strong>Reviewers for journals:</strong> TPAMI</p>
              </td>
            </tr>
		

          </tbody>
        </table>

          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Selected Awards</heading>
                <p align="justify"><strong>Shuimu Tsinghua Scholarship</strong></p>
		<p align="justify"><strong>China Postdoctoral Science Foundation General Program</strong></p>
<p align="justify"><strong>IEEE ISBI Best Student Paper Nomination</strong></p>              
</td>
            </tr>
          </tbody>
        </table>



          <br />
<br />
<a href="https://visitorbadge.io/status?path=bladedancer957.github.io">
  <img
    src="https://api.visitorbadge.io/api/visitors?path=bladedancer957.github.io&labelColor=%23dce775&countColor=%23263759&style=flat" />
</a>
<p align="left">
  <font size="2"><a href="https://people.eecs.berkeley.edu/~barron/">website template</a> </font>
</p>


        </td>
      </tr>


  </table>
</body>

</html>
