<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Duzhen Zhang (Âº†Á¨ÉÊåØ)</title>

  <meta name="author" content="Duzhen Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/avatar.jpg">


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Duzhen Zhang (Âº†Á¨ÉÊåØ)</name>
                  </p>

                  <p align="justify">
                    My name is Zhang Duzhen. 
                    I am a PhD student (2019.9 - 2024.6.(expected)) in artificial intelligence at <a href="http://english.ia.cas.cn/" target="_blank" rel="noopener">Institute of Automation, Chinese Academy of Sciences (CASIA)</a> advised by <a href="http://english.ia.cas.cn/en_sourcedb_ia/iaexpert/202310/t20231031_455698.html" target="_blank" rel="noopener">Bo Xu</a> and <a href="https://braincog.ai/~tielin.zhang/" target="_blank" rel="noopener">Tielin Zhang</a>. 
                    Prior to that, I received my bachelor degree in software engineering at <a href="https://www.sc.sdu.edu.cn/" target="_blank" rel="noopener">Shandong University</a> (2015.9. - 2019.6.).
                  </p>

                  <p align="justify">
                    Previously, my research interests included Emotion Analysis in Conversations, Incremental/Federated Learning and Its Applications in Information Extraction, and Brain¬≠-inspired Intelligence. Currently, I'm working on Continual Learning in LLMs, Large Multi-Modal Model, and Applications of LLMs.
                  </p>

                  <p align="justify">
                    I am looking for <font color="red"><strong>cooperation</strong></font>. Contact me if you are interested in the above topics.
                  </p>


                  <p align="justify">
                    I am actively seeking a <font color="red"><strong>postdoctoral research position</strong></font> as well. If you are interested in my experience, please feel free to contact me and let me know.
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:zhangduzhen2019@ia.ac.cn">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com.hk/citations?user=o0jlAfwAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.semanticscholar.org/author/Duzhen-Zhang/70123445">Semantic Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://dblp.uni-trier.de/pid/235/0398.html"> DBLP </a> &nbsp/&nbsp
                    <a href="https://github.com/BladeDancer957"> GitHub </a> &nbsp/&nbsp
                    <!-- <a href="https://weibo.com/u/6984959147"> Weibo </a> &nbsp/&nbsp -->
                    <!-- <a href="https://www.xiaohongshu.com/user/profile/5d3ecbf30000000011005f67"> XiaoHongShu </a> &nbsp/&nbsp -->
                    <a href="docs/Resume1.pdf"> Resum√©</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="#"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <table align="center" border="0" cellpadding="10" cellspacing="0" width="100%">
            <tbody>
              <tr>
                <td valign="middle" width="100%">
                  <heading>News </heading>
                </td>
              </tr>
            </tbody>
          </table>


          <ul>

            <li><span style="color: gray" size="6px">[2023.10]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2301.10292"><strong>SPN-GA</strong></a> is accepted by <strong>Machine Intelligence Research (MIR)</strong>.
            </li>

            <li><span style="color: gray" size="6px">[2023.10]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2310.14541"><strong>CPFD</strong></a> is accepted by <strong>EMNLP2023</strong> main conference as a long paper.
            </li>


            <li><span style="color: gray" size="6px">[2023.09]</span>
              üéâ One paper: <a href="https://arxiv.org/abs/2309.14078"><strong>ODE-RNN4RL</strong></a> is accepted by <strong>NeurIPS2023</strong>.
            </li>


            <li><span style="color: gray" size="6px">[2023.08]</span>
              üéâ One paper: <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615075"><strong>RDP</strong></a> is accepted by <strong>CIKM2023</strong> as a long paper. (<span style="color: red" size="6px"><strong>Oral</strong></span>)
            </li>


            <li><span style="color: gray" size="6px">[2023.05]</span>
              üéâ One paper: <a href="https://aclanthology.org/2023.acl-long.408/"><strong>DualGATs</strong></a> is accepted by <strong>ACL2023</strong> main conference as a long paper.
            </li>


            <li><span style="color: gray" size="6px">[2023.04]</span>
              üéâ One paper: <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591970"><strong>DLD</strong></a> is accepted by <strong>SIGIR2023</strong> as a short paper.
            </li>


            <li><span style="color: gray" size="6px">[2023.02]</span>
              üéâ One paper: <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html"><strong>FISS</strong></a> is accepted by <strong>CVPR2023</strong>.
            </li>


            <li><span style="color: gray" size="6px">[2023.01]</span>
              üéâ One paper: <a href="https://ieeexplore.ieee.org/abstract/document/10219015"><strong>SAMGN</strong></a> is accepted by <strong>IEEE TMM</strong>.
            </li>


    
            
    

            <!-- <li><span style="color: gray" size="6px">[2023.09]</span>
              üì£ <span style="color: red" size="6px"><strong>I will joint the CS department, SJTU as a research Assistant
                  Professor in Fall 2023.</strong></span>
            </li> -->

     
            <!-- <li><span style="color: gray" size="6px">[2023.08]</span>
              üë®üèª‚Äçüè´ I am honored to be an invited speaker at the <a
                href="https://sites.google.com/view/hands2023/home">HANDS
                workshop</a> at ICCV 2023.
            </li> -->

           
        
            <!-- <li><span style="color: gray" size="6px">[2022.10]</span>
              üë©üèª‚Äç‚ù§Ô∏è‚Äçüë®üèª I have taken the wonderful journey of marriage alongside my <a
                href="https://anran-xu.github.io">cherished wife</a>.
            </li> -->

       
        
         
            <!-- <li><span style="color: gray" size="6px">[2022.03]</span>
              üéâ Two paper were accepted by <strong>CVPR 2022</strong>:
              one <span style="color: red" size="6px"><strong>Oral</strong></span>, one poster. </li> -->
            <!-- <li><span style="color: gray" size="6px">[2021.11]</span>
                Unlock a new role: the reviewer</li> -->
         
  
          </ul>



          <table align="center" border="0" cellpadding="10" cellspacing="0" width="100%">
            <tbody>
              <tr>
                <td valign="middle" width="100%">
                  <heading>Research Overview </heading>
                </td>
              </tr>
            </tbody>
          </table>

  
          <ul>
              <li>
                  <strong>Emotion Analysis in Conversations:</strong>
                  <a href="https://aclanthology.org/2020.coling-main.392/">[COLING2020]</a>
                  , <a href="https://aclanthology.org/2022.coling-1.588/">[COLING2022]</a>
                  , <a href="https://ieeexplore.ieee.org/abstract/document/10219015">[IEEE TMM]</a>
                  , <a href="https://aclanthology.org/2023.acl-long.408/">[ACL2023]</a>
              </li>
              <li>
                <strong>Incremental/Federated Learning and Its Applications in Information Extraction:</strong>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html">[CVPR2023]</a>
                , <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591970">[SIGIR2023]</a>
                , <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615075">[CIKM2023]</a>
                , <a href="https://arxiv.org/abs/2310.14541">[EMNLP2023]</a>
            </li>

            <li>
              <strong>Brain¬≠-inspired Intelligence, Spiking Neural Networks, Reinforcement Learning:</strong>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19879">[AAAI2022]</a>
              , <a href="https://arxiv.org/abs/2204.07050">[IJCAI2022]</a>,
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25081">[AAAI2023]</a>,
              <a href="https://kns.cnki.net/kcms2/article/abstract?v=Eo9-C_M6tLkCF-ZWihqs-gCIpgHnxW86iit3wSHNIltOC7iK2Y94zRZ2zS7RvDtEOTn1_qMsCgFzehh8azYcoiZlW30SRWwpSVR8SD-HCNfuKEqQo27Ma6zQVKLtLAqWXXyhin-4VhkB27qAy_cFmQ==&uniplatform=NZKPT&language=CHS">[‰∫∫Â∑•Êô∫ËÉΩ]</a>
              , <a href="https://arxiv.org/abs/2301.10292">[MIR2023]</a>,
              <a href="https://arxiv.org/abs/2308.02557">[Preprint]</a>
              , <a href="https://arxiv.org/pdf/2309.14078">[NeurIPS2023]</a>
          </li>


          <li>
            <strong>Multi-Modal Learning, Large Pretrained Model:</strong>
            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547776">[ACM MM2022]</a>
            , <a href="https://link.springer.com/article/10.1007/s11633-022-1369-5">[MIR2022]</a>
        </li>
        
          

            </ul>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                </td>
                <p align="justify"><strong>* denotes equal contribution.</strong></p>
              </tr>
            </tbody>
      
          </table>



          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/11.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Tuning Synaptic Connections instead of Weights by Genetic Algorithm in Spiking Policy Network
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Tielin Zhang,
                  Shuncheng Jia,
                  Qingyu Wang,
                  Bo Xu
                  <br /><br />
                  <strong>Machine Intelligence Research (MIR)</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2301.10292">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/BladeDancer957/SPN-GA">Code</a>
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Learning from the interaction is the primary way biological agents know about the environment and themselves. Modern deep reinforcement learning (DRL) explores a computational approach to learning from interaction and has significantly progressed in solving various tasks. However, the powerful DRL is still far from biological agents in energy efficiency. Although the underlying mechanisms are not fully understood, we believe that the integration of spiking communication between neurons and biologically-plausible synaptic plasticity plays a prominent role. Following this biological intuition, we optimize a spiking policy network (SPN) by a genetic algorithm as an energy-efficient alternative to DRL. Our SPN mimics the sensorimotor neuron pathway of insects and communicates through event-based spikes. Inspired by biological research that the brain forms memories by forming new synaptic connections and rewires these connections based on new experiences, we tune the synaptic connections instead of weights in SPN to solve given tasks. Experimental results on several robotic control tasks show that our method can achieve the performance level of mainstream DRL methods and exhibit significantly higher energy efficiency.
                  </details>

                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/8.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Continual Named Entity Recognition without Catastrophic Forgetting
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Wei Cong,
                  Jiahua Dong,
                  Yahan Yu,
                  Xiuyi Chen,
                  <br>
                  Yonggang Zhang,
                  Zhen Fang
                  <br /><br />
                  <strong>EMNLP2023 (Main Conference Long Paper)</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2310.14541">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/BladeDancer957/CPFD">Code</a>&nbsp/&nbsp
                  <a href="posters/EMNLP2023_Poster.pdf">Poster</a>&nbsp/&nbsp
                  <a href="slides/EMNLP2023_Presentation.pdf">Slide</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Continual Named Entity Recognition (CNER) is a burgeoning area, which involves updating an existing model by incorporating new entity types sequentially. Nevertheless, continual learning approaches are often severely afflicted by catastrophic forgetting. This issue is intensified in CNER due to the consolidation of old entity types from previous steps into the non-entity type at each step, leading to what is known as the semantic shift problem of the non-entity type. In this paper, we introduce a pooled feature distillation loss that skillfully navigates the trade-off between retaining knowledge of old entity types and acquiring new ones, thereby more effectively mitigating the problem of catastrophic forgetting. Additionally, we develop a confidence-based pseudo-labeling for the non-entity type, \emph{i.e.,} predicting entity types using the old model to handle the semantic shift of the non-entity type. Following the pseudo-labeling process, we suggest an adaptive re-weighting type-balanced learning strategy to handle the issue of biased type distribution. We carried out comprehensive experiments on ten CNER settings using three different datasets. The results illustrate that our method significantly outperforms prior state-of-the-art approaches, registering an average improvement of 6.3% and 8.0% in Micro and Macro F1 scores, respectively.
                  </details>
                </td>
              </tr>


            
              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/13.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>ODE-based Recurrent Model-free Reinforcement Learning for POMDPs
                    </papertitle>
                  </a>
                  <br /><br />
                  Xuanle Zhao,
                  <strong>Duzhen Zhang</strong>,
                  Liyuan Han,
                  Tielin Zhang,
                  Bo Xu
                  <br /><br />
                  <strong>NeurIPS2023</strong>
                  <br>
                  <a href="https://arxiv.org/pdf/2309.14078">Paper</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularly-sampled time series.                
                  </details>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/16.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Attention-free Spikformer: Mixing Spike Sequences with Simple Linear Transforms
                    </papertitle>
                  </a>
                  <br /><br />
                  Qingyu Wang*,
                  <strong>Duzhen Zhang*</strong>,
                  Tielin Zhang,
                  Bo Xu
                  <br /><br />
                  <strong>Preprint</strong>
                  <br>
                  <a href="https://arxiv.org/abs/2308.02557">Paper</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    By integrating the self-attention capability and the biological properties of Spiking Neural Networks (SNNs), Spikformer applies the flourishing Transformer architecture to SNNs design. It introduces a Spiking Self-Attention (SSA) module to mix sparse visual features using spike-form Query, Key, and Value, resulting in the State-Of-The-Art (SOTA) performance on numerous datasets compared to previous SNN-like frameworks. In this paper, we demonstrate that the Spikformer architecture can be accelerated by replacing the SSA with an unparameterized Linear Transform (LT) such as Fourier and Wavelet transforms. These transforms are utilized to mix spike sequences, reducing the quadratic time complexity to log-linear time complexity. They alternate between the frequency and time domains to extract sparse visual features, showcasing powerful performance and efficiency. We conduct extensive experiments on image classification using both neuromorphic and static datasets. The results indicate that compared to the SOTA Spikformer with SSA, Spikformer with LT achieves higher Top-1 accuracy on neuromorphic datasets (i.e., CIFAR10-DVS and DVS128 Gesture) and comparable Top-1 accuracy on static datasets (i.e., CIFAR-10 and CIFAR-100). Furthermore, Spikformer with LT achieves approximately 29-51% improvement in training speed, 61-70% improvement in inference speed, and reduces memory usage by 4-26% due to not requiring learnable parameters.
                  </details>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/6.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Task Relation Distillation and Prototypical Pseudo Label for Continual Named Entity Recognition
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Hongliu Li,
                  Wei Cong,
                  Rongtao Xu,
                  <br>
                  Jiahua Dong,
                  Xiuyi Chen
                  <br /><br />
                  <strong>CIKM2023 (Long Paper)</strong> (<font color="red"><strong>Oral</strong></font>)
                  <br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3583780.3615075">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/BladeDancer957/INER_RDP">Code</a>&nbsp/&nbsp
                  <a href="slides/CIKM2023_Presentation.pdf">Slide</a> 
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Incremental Named Entity Recognition (INER) involves the sequential learning of new entity types without accessing the training data of previously learned types. However, INER faces the challenge of catastrophic forgetting specific for incremental learning, further aggravated by background shift (i.e., old and future entity types are labeled as the non-entity type in the current task). To address these challenges, we propose a method called task Relation Distillation and Prototypical pseudo label (RDP) for INER. Specifically, to tackle catastrophic forgetting, we introduce a task relation distillation scheme that serves two purposes: 1) ensuring inter-task semantic consistency across different incremental learning tasks by minimizing inter-task relation distillation loss, and 2) enhancing the model's prediction confidence by minimizing intra-task self-entropy loss. Simultaneously, to mitigate background shift, we develop a prototypical pseudo label strategy that distinguishes old entity types from the current non-entity type using the old model. This strategy generates high-quality pseudo labels by measuring the distances between token embeddings and type-wise prototypes. We conducted extensive experiments on ten INER settings of three benchmark datasets (i.e., CoNLL2003, I2B2, and OntoNotes5). The results demonstrate that our method achieves significant improvements over the previous state-of-the-art methods, with an average increase of 6.08% in Micro F1 score and 7.71% in Macro F1 score.
                  </details>
                </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/4.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>DualGATs:Dual Graph Attention Networks for Emotion Recognition in Conversations
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Feilong Chen,
                  Xiuyi Chen
                  <br /><br />
                  <strong>ACL2023 (Main Conference Long Paper)</strong>
                  <br>
                  <a href="https://aclanthology.org/2023.acl-long.408/">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/BladeDancer957/DualGATs">Code</a>&nbsp/&nbsp
                  <a href="posters/ACL2023_Poster.pdf">Poster</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT) module to incorporate discourse structural information by analyzing the discourse dependencies between utterances. Additionally, we develop a Speaker-aware GAT (SpkGAT) module to incorporate speaker-aware contextual information by considering the speaker dependencies between utterances. Furthermore, we design an interaction module that facilitates the integration of the DisGAT and SpkGAT modules, enabling the effective interchange of relevant information between the two modules. We extensively evaluate our method on four datasets, and experimental results demonstrate that our proposed DualGATs surpass state-of-the-art baselines on the majority of the datasets.                 
                  </details>
                </td>
              </tr>



              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/5.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Decomposing Logits Distillation for Incremental Named Entity Recognition
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Yahan Yu,
                  Feilong Chen,
                  Xiuyi Chen
                  <br /><br />
                  <strong>SIGIR2023 (Short Paper)</strong>
                  <br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3539618.3591970">Paper</a>&nbsp/&nbsp
                  <a href="posters/SIGIR_Poster.pdf">Poster</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Incremental Named Entity Recognition (INER) aims to continually train a model with new data, recognizing emerging entity types without forgetting previously learned ones. Prior INER methods have shown that Logits Distillation (LD), which involves preserving predicted logits via knowledge distillation, effectively alleviates this challenging issue. In this paper, we discover that a predicted logit can be decomposed into two terms that measure the likelihood of an input token belonging to a specific entity type or not. However, the traditional LD only preserves the sum of these two terms without considering the change in each component. To explicitly constrain each term, we propose a novel Decomposing Logits Distillation (DLD) method, enhancing the model's ability to retain old knowledge and mitigate catastrophic forgetting. Moreover, DLD is model-agnostic and easy to implement. Extensive experiments show that DLD consistently improves the performance of state-of-the-art INER methods across ten INER settings in three datasets.                 
                  </details>
                  </td>
              </tr>


              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/7.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Federated Incremental Semantic Segmentation
                    </papertitle>
                  </a>
                  <br /><br />
                  Jiahua Dong*,
                  <strong>Duzhen Zhang*</strong>,
                  Yang Cong,
                  Wei Cong,
                  <br>
                  Henghui Ding,
                  Dengxin Dai
                  <br /><br />
                  <strong>CVPR2023</strong>
                  <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Federated_Incremental_Semantic_Segmentation_CVPR_2023_paper.html">Paper</a>&nbsp/&nbsp
                  <a href="https://github.com/JiahuaDong/FISS">Code</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Federated learning-based semantic segmentation (FSS) has drawn widespread attention via decentralized training on local clients. However, most FSS models assume categories are fxed in advance, thus heavily undergoing forgetting on old categories in practical applications where local clients receive new categories incrementally while have no memory storage to access old classes. Moreover, new clients collecting novel classes may join in the global training of FSS, which further exacerbates catastrophic forgetting. To surmount the above challenges, we propose a Forgetting-Balanced Learning (FBL) model to address heterogeneous forgetting on old classes from both intra-client and inter-client aspects. Specifically, under the guidance of pseudo labels generated via adaptive class-balanced pseudo labeling, we develop a forgetting-balanced semantic compensation loss and a forgetting-balanced relation consistency loss to rectify intra-client heterogeneous forgetting of old categories with background shift. It performs balanced gradient propagation and relation consistency distillation within local clients. Moreover, to tackle heterogeneous forgetting from inter-client aspect, we propose a task transition monitor. It can identify new classes under privacy protection and store the latest old global model for relation distillation. Qualitative experiments reveal large improvement of our model against comparison methods. The code is available at https://github.com/JiahuaDong/FISS.         
                  </details>
                  </td>
              </tr>

              
              <tr>
                <td style="padding:25px;width:35%;vertical-align:middle">
                  <div class="one">
                    <img src='images/3.png' width="250">
                  </div>
                </td>
                <td style="padding:25px;width:70%;vertical-align:middle">
                  <a href="">
                    <papertitle>Structure Aware Multi-Graph Network for Multi-Modal Emotion Recognition in Conversations
                    </papertitle>
                  </a>
                  <br /><br />
                  <strong>Duzhen Zhang</strong>,
                  Feilong Chen,
                  Jianlong Chang,
                  Xiuyi Chen,
                  Qi Tian
                  <br /><br />
                  <strong>IEEE TMM</strong>
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/10219015">Paper</a>
      
                  <details align="justify">
                    <summary>
                      Details
                    </summary>
                    Multi-Modal Emotion Recognition in Conversations (MMERC) is an increasingly active research field that leverages multi-modal signals to understand the feelings behind each utterance. Modeling contextual interactions and multi-modal fusion lie at the heart of this field, with graph-based models recently being widely used for MMERC to capture global multi-modal contextual information. However, these models generally mix all modality representations in a single graph, and utterances in each modality are fully connected, potentially ignoring three problems:(1) the heterogeneity of the multi-modal context, (2) the redundancy of contextual information, and (3) over-smoothing of the graph networks. To address these problems, we propose a Structure Aware Multi-Graph Network (SAMGN) for MMERC. Specifically, we construct multiple modality-specific graphs to model the heterogeneity of the multi-modal context. Instead of fully connecting the utterances in each modality, we design a structure learning module that determines whether edges exist between the utterances. This module reduces redundancy by forcing each utterance to focus on the contextual ones that contribute to its emotion recognition, acting like a message propagating reducer to alleviate over-smoothing. Then, we develop the SAMGN via Dual-Stream Propagation (DSP), which contains two propagation streams, i.e., intra- and inter-modal, performed in parallel to aggregate the heterogeneous modality information from multi-graphs. DSP also contains a gating unit that adaptively integrates the co-occurrence information from the above two propagations for emotion recognition. Experiments on two popular MMERC datasets demonstrate that SAMGN achieves new State-Of-The-Art (SOTA) results.               
                  </details>
                </td>
                  </tr>


                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/17.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>ÁîüÁâ©ÁªìÊûÑÂêØÂèëÂü∫Êú¨ÁΩëÁªúÁÆóÂ≠êÂä©ÂäõÁ±ªËÑëÊô∫ËÉΩÁ†îÁ©∂
                        </papertitle>
                      </a>
                      <br /><br />
                      <strong>Âº†Á¨ÉÊåØ</strong>,
                      Á®ãÁøî,
                      ÁéãÂ≤©Êùæ,
                      Âº†Êñ∞Ë¥∫,
                      Âº†ÈìÅÊûó,
                      Êùú‰πÖÊûó,
                      ÂæêÊ≥¢
                      <br /><br />
                      <strong>‰∫∫Â∑•Êô∫ËÉΩ</strong>
                      <br>
                      <a href="https://kns.cnki.net/kcms2/article/abstract?v=Eo9-C_M6tLkCF-ZWihqs-gCIpgHnxW86iit3wSHNIltOC7iK2Y94zRZ2zS7RvDtEOTn1_qMsCgFzehh8azYcoiZlW30SRWwpSVR8SD-HCNfuKEqQo27Ma6zQVKLtLAqWXXyhin-4VhkB27qAy_cFmQ==&uniplatform=NZKPT&language=CHS">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        Á±ªËÑëÊô∫ËÉΩÁ†îÁ©∂Ê∑±ÂÖ•‰∫§ÂèâËÑëÁßëÂ≠¶Âíå‰∫∫Â∑•Êô∫ËÉΩ, Êó®Âú®‰ªéËÑëÁßëÂ≠¶‰∏≠Ê±≤ÂèñÁªìÊûÑ„ÄÅÂäüËÉΩ„ÄÅÊú∫Âà∂Á≠âÊñπÈù¢ÁöÑÁÅµÊÑü, Áî®‰ª•ÂêØÂèë‰∫∫Â∑•Êô∫ËÉΩËΩØÁ°¨‰ª∂Á†îÁ©∂„ÄÇÊú¨ÊñáËÅöÁÑ¶ÁîüÁâ©ÁªìÊûÑ, ÈáçÁÇπÊÄªÁªìÁ•ûÁªè‰æßÂêë‰∫§‰∫í„ÄÅÁîüÁâ©ÂΩ©Á•®ÁΩëÁªúÂÅáËÆæ„ÄÅMot‰ª∂Êû∂ÊûÑÁöÑÁªìÊûÑËÆæËÆ°‰∏≠„ÄÇÊú™Êù•, ÈöèÁùÄÂ§öÂ∞∫Â∫¶ÂíåÂ§öÁ±ªÂûãÁîüÁâ©ÁΩëÁªúÁªÑÂõæË∞±ÁöÑÁªòÂà∂ÔºåË∂äÊù•Ë∂äÂ§öÁîüÁâ©ÁªìÊûÑÂêØÂèëÁöÑÁΩëÁªúÂü∫Êú¨ÁÆóÂ≠êÂèØ‰ª•Ë¢´ÊäΩÊèêÂá∫Êù•Âπ∂ÊåÅÁª≠Êé®Âä®Á±ªËÑëÊô∫ËÉΩÁöÑÂàõÊñ∞ÂèëÂ±ï„ÄÇ
                      </details>
                      </td>
                  </tr>



                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/15.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>Complex Dynamic Neurons Improved Spiking Transformer Network for Efficient Automatic Speech Recognition
                        </papertitle>
                      </a>
                      <br /><br />
                      Qingyu Wang,
                      Tielin Zhang,
                      Minglun Han,
                      Yi Wang,
                      <strong>Duzhen Zhang</strong>,
                      Bo Xu
                      <br /><br />
                      <strong>AAAI2023</strong>
                      <br>
                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25081">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        The spiking neural network (SNN) using leaky-integrated-and-fire (LIF) neurons has been commonly used in automatic speech recognition (ASR) tasks. However, the LIF neuron is still relatively simple compared to that in the biological brain. Further research on more types of neurons with different scales of neuronal dynamics is necessary. Here we introduce four types of neuronal dynamics to post-process the sequential patterns generated from the spiking transformer to get the complex dynamic neuron improved spiking transformer neural network (DyTr-SNN). We found that the DyTr-SNN could handle the non-toy automatic speech recognition task well, representing a lower phoneme error rate, lower computational cost, and higher robustness. These results indicate that the further cooperation of SNNs and neural dynamics at the neuron and network scales might have much in store for the future, especially on the ASR tasks.
                      </details>
                    </td>
                  </tr>



            
                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/12.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>VLP: A Survey on Vision-¬≠Language Pre¬≠-training
                        </papertitle>
                      </a>
                      <br /><br />
                      Feilong Chen*,
                      <strong>Duzhen Zhang*</strong>,
                      Minglun Han,
                      Xiuyi Chen,
                      <br>
                      Jing Shi,
                      Shuang Xu,
                      Bo Xu
                      <br /><br />
                      <strong>Machine Intelligence Research (MIR)</strong>
                      <br>
                      <a href="https://link.springer.com/article/10.1007/s11633-022-1369-5">Paper</a>
               
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown that they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances in five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey focused on VLP. We hope that this survey can shed light on future research in the VLP field.                      </details>
                      </td>
                  </tr>



                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/2.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>TSAM: A Two-Stream Attention Model for Causal Emotion Entailment
                        </papertitle>
                      </a>
                      <br /><br />
                      <strong>Duzhen Zhang</strong>,
                      Zhen Yang,
                      Fandong Meng,
                      Xiuyi Chen,
                      Jie Zhou
                      <br /><br />
                      <strong>COLING2022 (Long Paper)</strong>  (<font color="red"><strong>Oral</strong></font>)
                      <br>
                      <a href="https://aclanthology.org/2022.coling-1.588/">Paper</a>&nbsp/&nbsp
                      <a href="https://github.com/BladeDancer957/TSAM">Code</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        Causal Emotion Entailment (CEE) aims to discover the potential causes behind an emotion in a conversational utterance. Previous works formalize CEE as independent utterance pair classification problems, with emotion and speaker information neglected. From a new perspective, this paper considers CEE in a joint framework. We classify multiple utterances synchronously to capture the correlations between utterances in a global view and propose a Two-Stream Attention Model (TSAM) to effectively model the speaker‚Äôs emotional influences in the conversational history. Specifically, the TSAM comprises three modules: Emotion Attention Network (EAN), Speaker Attention Network (SAN), and interaction module. The EAN and SAN incorporate emotion and speaker information in parallel, and the subsequent interaction module effectively interchanges relevant information between the EAN and SAN via a mutual BiAffine transformation. Extensive experimental results demonstrate that our model achieves new State-Of-The-Art (SOTA) performance and outperforms baselines remarkably.                      </details>
                      </td>
                  </tr>
                  

                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/14.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>Unsupervised and Pseudo-Supervised Vision-Language Alignment in Visual Dialog
                        </papertitle>
                      </a>
                      <br /><br />
                      Feilong Chen,
                      <strong>Duzhen Zhang</strong>,
                      Xiuyi Chen,
                      Jing Shi,
                      Shuang Xu,
                      Bo Xu
                      <br /><br />
                      <strong>ACM MM2022</strong>
                      <br>
                      <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547776">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        Visual dialog requires models to give reasonable answers according to a series of coherent questions and related visual concepts in images. However, most current work either focuses on attention-based fusion or pre-training on large-scale image-text pairs, ignoring the critical role of explicit vision-language alignment in visual dialog. To remedy this defect, we propose a novel unsupervised and pseudo-supervised vision-language alignment approach for visual dialog (AlignVD). Firstly, AlginVD utilizes the visual and dialog encoder to represent images and dialogs. Then, it explicitly aligns visual concepts with textual semantics via unsupervised and pseudo-supervised vision-language alignment (UVLA and PVLA). Specifically, UVLA utilizes a graph autoencoder, while PVLA uses dialog-guided visual grounding to conduct alignment. Finally, based on the aligned visual and textual representations, AlignVD gives a reasonable answer to the question via the cross-modal decoder. Extensive experiments on two large-scale visual dialog datasets have demonstrated the effectiveness of vision-language alignment, and our proposed AlignVD achieves new state-of-the-art results. In addition, our single model has won first place on the visual dialog challenge leaderboard with a NDCG metric of 78.70, surpassing the previous best ensemble model by about 1 point.
                      </details>
                    </td>
                  </tr>



                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/10.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>Recent Advances and New Frontiers in Spiking Neural Networks
                        </papertitle>
                      </a>
                      <br /><br />
                      <strong>Duzhen Zhang</strong>,
                      Shuncheng Jia,
                      Qingyu Wang
                      <br /><br />
                      <strong>IJCAI2022</strong>
                      <br>
                      <a href="https://arxiv.org/abs/2204.07050">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        In recent years, spiking neural networks (SNNs) have received extensive attention in brain-inspired intelligence due to their rich spatially-temporal dynamics, various encoding methods, and event-driven characteristics that naturally fit the neuromorphic hardware. With the development of SNNs, brain-inspired intelligence, an emerging research field inspired by brain science achievements and aiming at artificial general intelligence, is becoming hot. This paper reviews recent advances and discusses new frontiers in SNNs from five major research topics, including essential elements (i.e., spiking neuron models, encoding methods, and topology structures), neuromorphic datasets, optimization algorithms, software, and hardware frameworks. We hope our survey can help researchers understand SNNs better and inspire new works to advance this field.
                      </details>
                      </td>
                  </tr>




                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/9.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>Multi¬≠scale Dynamic Coding improved Spiking Actor Network for Reinforcement Learning
                        </papertitle>
                      </a>
                      <br /><br />
                      <strong>Duzhen Zhang</strong>,
                      Tielin Zhang,
                      Shuncheng Jia,
                      Bo Xu
                      <br /><br />
                      <strong>AAAI2022</strong>  (<font color="red"><strong>Oral</strong></font>)
                      <br>
                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19879">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        With the help of deep neural networks (DNNs), deep reinforcement learning (DRL) has achieved great success on many complex tasks, from games to robotic control. Compared to DNNs with partial brain-inspired structures and functions, spiking neural networks (SNNs) consider more biological features, including spiking neurons with complex dynamics and learning paradigms with biologically plausible plasticity principles. Inspired by the efficient computation of cell assembly in the biological brain, whereby memory-based coding is much more complex than readout, we propose a multiscale dynamic coding improved spiking actor network (MDC-SAN) for reinforcement learning to achieve effective decision-making. The population coding at the network scale is integrated with the dynamic neurons coding (containing 2nd-order neuronal dynamics) at the neuron scale towards a powerful spatial-temporal state representation. Extensive experimental results show that our MDC-SAN performs better than its counterpart deep actor network (based on DNNs) on four continuous control tasks from OpenAI gym. We think this is a significant attempt to improve SNNs from the perspective of efficient coding towards effective decision-making, just like that in biological networks.                 
                      </details>
                      </td>
                  </tr>



                  <tr>
                    <td style="padding:25px;width:35%;vertical-align:middle">
                      <div class="one">
                        <img src='images/1.png' width="250">
                      </div>
                    </td>
                    <td style="padding:25px;width:70%;vertical-align:middle">
                      <a href="">
                        <papertitle>Knowledge Aware Emotion Recognition in Textual Conversations via Multi-Task Incremental Transformer
                        </papertitle>
                      </a>
                      <br /><br />
                      <strong>Duzhen Zhang</strong>,
                      Xiuyi Chen,
                      Shuang Xu,
                      Bo Xu
                      <br /><br />
                      <strong>COLING2020 (Long Paper)</strong>  (<font color="red"><strong>Oral</strong></font>)
                      <br>
                      <a href="https://aclanthology.org/2020.coling-main.392/">Paper</a>
          
                      <details align="justify">
                        <summary>
                          Details
                        </summary>
                        Emotion recognition in textual conversations (ERTC) plays an important role in a wide range of applications, such as opinion mining, recommender systems, and so on. ERTC, however, is a challenging task. For one thing, speakers often rely on the context and commonsense knowledge to express emotions; for another, most utterances contain neutral emotion in conversations, as a result, the confusion between a few non-neutral utterances and much more neutral ones restrains the emotion recognition performance. In this paper, we propose a novel Knowledge Aware Incremental Transformer with Multi-task Learning (KAITML) to address these challenges. Firstly, we devise a dual-level graph attention mechanism to leverage commonsense knowledge, which augments the semantic information of the utterance. Then we apply the Incremental Transformer to encode multi-turn contextual utterances. Moreover, we are the first to introduce multi-task learning to alleviate the aforementioned confusion and thus further improve the emotion recognition performance. Extensive experimental results show that our KAITML model outperforms the state-of-the-art models across five benchmark datasets.                     
                      </details>
                      </td>
                  </tr>

          </table>


          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Services</heading>
              </td>
              <p align="justify"><strong>Conference Reviewers</strong></p>
            </tr>
          </tbody>
        </table>
<ul>
  <li><autocolor>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2024</autocolor></li>
  <li><autocolor>The AAAI Conference on Artificial Intelligence (AAAI) 2023, 2024</autocolor></li>
  <li><autocolor>Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023</autocolor></li>
  <li><autocolor>Annual Meeting of the Association for Computational Linguistics (ACL) 2023</autocolor></li>
  <li><autocolor>International Joint Conference on Artificial Intelligence (IJCAI) 2023</autocolor></li>  

</ul>

<!-- <h4 style="margin:0 10px 0;">Journal Reviewers</h4>

<ul style="margin:0 0 20px;">
  <li><a href="https://www.computer.org/csdl/journal/tp"><autocolor>Neurocomputing</autocolor></a></li>
  <li><a href="https://www.springer.com/journal/11263"><autocolor>Pattern Recognition</autocolor></a></li>
  <li><a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-image-processing"><autocolor>IEEE Transactions on Knowledge and Data Engineering (TKDE)</autocolor></a></li>
  <li><a href="https://www.computer.org/csdl/journal/tk"><autocolor>IET Image Processing</autocolor></a></li>
  <li><a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia"><autocolor>IEEE Transactions on Big Data</autocolor></a></li>
  <li><a href="https://ieee-cas.org/publications/ieee-transactions-circuits-and-systems-video-technology"><autocolor>Computer Science Review</autocolor></a></li>
  <li><a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems"><autocolor>Cybernetics and Systems</autocolor></a></li>
  <li><a href="https://dl.acm.org/journal/tomm"><autocolor>Transactions on Systems, Man and Cybernetics: Systems</autocolor></a></li>
  
  <li><a href="https://www.journals.elsevier.com/neural-networks"><autocolor>Neural Networks</autocolor></a></li>
  <li><a href="https://www.springer.com/journal/10994"><autocolor>Machine Learning</autocolor></a></li>
  <li><a href="https://www.journals.elsevier.com/information-processing-and-management"><autocolor>Information Processing and Management</autocolor></a></li>
  <li><a href="https://www.springer.com/journal/11063"><autocolor>Neural Processing Letters</autocolor></a></li>
  <li><a href="https://link.springer.com/journal/11042"><autocolor>Multimedia Tools and Applications</autocolor></a></li>
  <li><a href="https://ieeeaccess.ieee.org/"><autocolor>IEEE Access</autocolor></a></li>
  <li><a href="http://cjc.ict.ac.cn/"><autocolor>Chinese Journal of Computers</autocolor></a></li>
</ul> 

-->



          <br />
<br />
<a href="https://visitorbadge.io/status?path=bladedancer957.github.io">
  <img
    src="https://api.visitorbadge.io/api/visitors?path=bladedancer957.github.io&labelColor=%23dce775&countColor=%23263759&style=flat" />
</a>
<p align="left">
  <font size="2"><a href="https://people.eecs.berkeley.edu/~barron/">website template</a> </font>
</p>


        </td>
      </tr>


  </table>
</body>

</html>